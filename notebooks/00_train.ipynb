{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2180b18e",
   "metadata": {},
   "source": [
    "# üèéÔ∏è Gemma-3 F1 Expert Training Notebook\n",
    "\n",
    "This notebook provides a complete walkthrough for fine-tuning Google's Gemma-3 model into a Formula 1 knowledge expert using LoRA (Low-Rank Adaptation).\n",
    "\n",
    "## üìã What you'll learn:\n",
    "- How to collect F1 data from APIs and RSS feeds\n",
    "- Dataset preparation for instruction tuning\n",
    "- LoRA fine-tuning with Unsloth for memory efficiency\n",
    "- Model evaluation and testing\n",
    "- Deployment options\n",
    "\n",
    "## üöÄ Quick Start:\n",
    "1. Run all cells in order (Runtime ‚Üí Run all)\n",
    "2. Total time: ~30-45 minutes on a T4 GPU\n",
    "3. Model will be saved and ready for use\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c55616b6",
   "metadata": {},
   "source": [
    "## 1Ô∏è‚É£ Environment Setup\n",
    "\n",
    "First, let's set up the environment with all necessary dependencies."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aae487b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check GPU availability\n",
    "import torch\n",
    "print(f\"üñ•Ô∏è  GPU Available: {torch.cuda.is_available()}\")\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"GPU Name: {torch.cuda.get_device_name(0)}\")\n",
    "    print(f\"GPU Memory: {torch.cuda.get_device_properties(0).total_memory / 1024**3:.1f} GB\")\n",
    "else:\n",
    "    print(\"‚ö†Ô∏è  No GPU detected. Training will be slow.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79f78165",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install required packages\n",
    "!pip install -q torch transformers datasets accelerate peft bitsandbytes trl\n",
    "!pip install -q \"unsloth[colab-new] @ git+https://github.com/unslothai/unsloth.git\"\n",
    "!pip install -q requests feedparser tqdm pandas numpy\n",
    "\n",
    "print(\"‚úÖ Dependencies installed!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79568ac3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import libraries\n",
    "import json\n",
    "import time\n",
    "import random\n",
    "import requests\n",
    "import feedparser\n",
    "from datetime import datetime, timedelta\n",
    "from pathlib import Path\n",
    "from tqdm import tqdm\n",
    "\n",
    "import torch\n",
    "from datasets import Dataset\n",
    "from transformers import TrainingArguments\n",
    "from unsloth import FastLanguageModel\n",
    "from trl import SFTTrainer\n",
    "\n",
    "print(\"üìö Libraries imported successfully!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98ad2292",
   "metadata": {},
   "source": [
    "## 2Ô∏è‚É£ Data Collection\n",
    "\n",
    "Let's collect Formula 1 data from the Jolpica API and create explanatory content."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32eb930e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create data directory\n",
    "!mkdir -p data\n",
    "\n",
    "# Jolpica F1 API client\n",
    "class QuickF1DataCollector:\n",
    "    \"\"\"Simplified F1 data collector for notebook use.\"\"\"\n",
    "    \n",
    "    BASE_URL = \"https://api.jolpi.ca/ergast/f1\"\n",
    "    \n",
    "    def __init__(self):\n",
    "        self.session = requests.Session()\n",
    "        self.session.headers.update({\n",
    "            'User-Agent': 'gemma-f1-expert-colab/1.0'\n",
    "        })\n",
    "    \n",
    "    def get_recent_data(self, years=3):\n",
    "        \"\"\"Get recent F1 data (last few years for speed).\"\"\"\n",
    "        current_year = datetime.now().year\n",
    "        start_year = current_year - years\n",
    "        \n",
    "        data = []\n",
    "        \n",
    "        for year in range(start_year, current_year + 1):\n",
    "            print(f\"Collecting {year} data...\")\n",
    "            \n",
    "            try:\n",
    "                # Get season races\n",
    "                time.sleep(0.2)  # Rate limiting\n",
    "                races_url = f\"{self.BASE_URL}/{year}.json\"\n",
    "                races_resp = self.session.get(races_url, timeout=10)\n",
    "                races_data = races_resp.json()\n",
    "                \n",
    "                races = races_data.get(\"MRData\", {}).get(\"RaceTable\", {}).get(\"Races\", [])\n",
    "                \n",
    "                for race in races[:5]:  # Limit to first 5 races per year for speed\n",
    "                    round_num = race[\"round\"]\n",
    "                    race_name = race[\"raceName\"]\n",
    "                    \n",
    "                    # Get race results\n",
    "                    time.sleep(0.2)\n",
    "                    results_url = f\"{self.BASE_URL}/{year}/{round_num}/results.json\"\n",
    "                    results_resp = self.session.get(results_url, timeout=10)\n",
    "                    results_data = results_resp.json()\n",
    "                    \n",
    "                    race_results = results_data.get(\"MRData\", {}).get(\"RaceTable\", {}).get(\"Races\", [])\n",
    "                    \n",
    "                    if race_results and race_results[0].get(\"Results\"):\n",
    "                        winner = race_results[0][\"Results\"][0]\n",
    "                        driver_name = f\"{winner['Driver']['givenName']} {winner['Driver']['familyName']}\"\n",
    "                        constructor = winner[\"Constructor\"][\"name\"]\n",
    "                        \n",
    "                        data.append({\n",
    "                            \"year\": year,\n",
    "                            \"race\": race_name,\n",
    "                            \"winner\": driver_name,\n",
    "                            \"constructor\": constructor,\n",
    "                            \"round\": round_num\n",
    "                        })\n",
    "                \n",
    "            except Exception as e:\n",
    "                print(f\"Error collecting {year} data: {e}\")\n",
    "                continue\n",
    "        \n",
    "        return data\n",
    "\n",
    "# Collect F1 data\n",
    "print(\"üèéÔ∏è  Collecting F1 race data...\")\n",
    "collector = QuickF1DataCollector()\n",
    "f1_data = collector.get_recent_data(years=3)\n",
    "\n",
    "print(f\"‚úÖ Collected {len(f1_data)} race results\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c8e27a7",
   "metadata": {},
   "source": [
    "## 3Ô∏è‚É£ Dataset Creation\n",
    "\n",
    "Now let's create question-answer pairs for training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25bee646",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_f1_dataset(race_data):\n",
    "    \"\"\"Create Q-A dataset from race data and explanatory content.\"\"\"\n",
    "    qa_pairs = []\n",
    "    \n",
    "    # Factual questions from race data\n",
    "    for race in race_data:\n",
    "        year = race[\"year\"]\n",
    "        race_name = race[\"race\"]\n",
    "        winner = race[\"winner\"]\n",
    "        constructor = race[\"constructor\"]\n",
    "        \n",
    "        qa_pairs.extend([\n",
    "            {\n",
    "                \"question\": f\"Who won the {year} {race_name}?\",\n",
    "                \"answer\": f\"{winner} won the {year} {race_name} driving for {constructor}.\",\n",
    "                \"type\": \"factual\",\n",
    "                \"category\": \"race_winner\"\n",
    "            },\n",
    "            {\n",
    "                \"question\": f\"Which team won the {year} {race_name}?\",\n",
    "                \"answer\": f\"{constructor} won the {year} {race_name} with {winner}.\",\n",
    "                \"type\": \"factual\",\n",
    "                \"category\": \"constructor_winner\"\n",
    "            }\n",
    "        ])\n",
    "    \n",
    "    # Explanatory content\n",
    "    explanatory_qa = [\n",
    "        {\n",
    "            \"question\": \"How does DRS work in Formula 1?\",\n",
    "            \"answer\": \"DRS (Drag Reduction System) is a movable rear wing flap that drivers can activate to reduce aerodynamic drag. It can only be used in designated DRS zones when a driver is within one second of the car ahead during races. When activated, the rear wing opens, reducing downforce and allowing higher straight-line speeds for overtaking. DRS is automatically disabled in wet conditions for safety.\",\n",
    "            \"type\": \"explanatory\",\n",
    "            \"category\": \"technical\"\n",
    "        },\n",
    "        {\n",
    "            \"question\": \"What are the different F1 tyre compounds?\",\n",
    "            \"answer\": \"F1 uses three dry tyre compounds per weekend: soft (red sidewall), medium (yellow sidewall), and hard (white sidewall). Softer compounds provide more grip but degrade faster, while harder compounds last longer but offer less grip. Teams must use at least two different compounds during a race. There are also intermediate (green) and wet (blue) tyres for rain conditions.\",\n",
    "            \"type\": \"explanatory\",\n",
    "            \"category\": \"technical\"\n",
    "        },\n",
    "        {\n",
    "            \"question\": \"How does F1 qualifying work?\",\n",
    "            \"answer\": \"F1 qualifying consists of three knockout sessions: Q1 (18 minutes), Q2 (15 minutes), and Q3 (12 minutes). In Q1, the five slowest drivers are eliminated. In Q2, another five are eliminated, and the remaining drivers' tyre choice determines their race start tyres. Q3 features the top 10 drivers competing for pole position.\",\n",
    "            \"type\": \"explanatory\",\n",
    "            \"category\": \"format\"\n",
    "        },\n",
    "        {\n",
    "            \"question\": \"What is the current F1 points system?\",\n",
    "            \"answer\": \"The current F1 points system awards points to the top 10 finishers: 25-18-15-12-10-8-6-4-2-1 points for positions 1st through 10th respectively. An additional point is awarded for the fastest lap, but only if the driver finishes in the points (top 10). Both the Drivers' Championship and Constructors' Championship use this system.\",\n",
    "            \"type\": \"explanatory\",\n",
    "            \"category\": \"rules\"\n",
    "        },\n",
    "        {\n",
    "            \"question\": \"What happens during an F1 pit stop?\",\n",
    "            \"answer\": \"During a pit stop, teams can change tyres, adjust front wing angles, and make minor repairs. A typical pit stop takes 2-3 seconds for a tyre change. Teams can make unlimited pit stops during a race, but drivers must use at least two different tyre compounds. Pit stops are crucial for strategy, timing them to minimize time loss while gaining track position or fresh tyres.\",\n",
    "            \"type\": \"explanatory\",\n",
    "            \"category\": \"strategy\"\n",
    "        },\n",
    "        {\n",
    "            \"question\": \"How do F1 safety cars work?\",\n",
    "            \"answer\": \"Safety cars are deployed when there's a hazard on track that requires marshals to work safely. All drivers must slow down and follow the safety car in single file, with no overtaking allowed. This bunches up the field and allows safe track clearing. Racing resumes when the safety car returns to the pits. Virtual Safety Cars (VSC) require drivers to maintain specific lap times without a physical safety car.\",\n",
    "            \"type\": \"explanatory\",\n",
    "            \"category\": \"safety\"\n",
    "        },\n",
    "        {\n",
    "            \"question\": \"Who is Lewis Hamilton?\",\n",
    "            \"answer\": \"Lewis Hamilton is a British Formula 1 driver and seven-time World Champion, tied for the most championships in F1 history with Michael Schumacher. He drives for Mercedes-AMG and holds the record for most race wins, pole positions, and podium finishes. Hamilton made his F1 debut in 2007 with McLaren and won his first championship in 2008.\",\n",
    "            \"type\": \"factual\",\n",
    "            \"category\": \"driver_info\"\n",
    "        },\n",
    "        {\n",
    "            \"question\": \"What is the Monaco Grand Prix?\",\n",
    "            \"answer\": \"The Monaco Grand Prix is one of Formula 1's most prestigious races, held annually on the streets of Monte Carlo in Monaco. The circuit is known for its tight, twisty layout with minimal overtaking opportunities, making qualifying position crucial. It's part of the Triple Crown of Motorsport along with the Indianapolis 500 and 24 Hours of Le Mans.\",\n",
    "            \"type\": \"explanatory\",\n",
    "            \"category\": \"circuit_info\"\n",
    "        },\n",
    "        {\n",
    "            \"question\": \"How long is an F1 race?\",\n",
    "            \"answer\": \"An F1 race is scheduled to run for a specific number of laps depending on the circuit, designed to last approximately 1 hour and 45 minutes. However, races have a maximum time limit of 2 hours for safety reasons. If weather delays occur, the race may be shortened. The minimum distance for a full race is 75% of the scheduled distance.\",\n",
    "            \"type\": \"explanatory\",\n",
    "            \"category\": \"rules\"\n",
    "        },\n",
    "        {\n",
    "            \"question\": \"What is Formula 1?\",\n",
    "            \"answer\": \"Formula 1 is the highest class of international auto racing for single-seater formula racing cars. It features a series of races called Grands Prix held on purpose-built circuits and closed public roads around the world. F1 cars are the fastest regulated road-course racing cars in the world, featuring advanced aerodynamics, hybrid power units, and cutting-edge technology.\",\n",
    "            \"type\": \"explanatory\",\n",
    "            \"category\": \"general\"\n",
    "        }\n",
    "    ]\n",
    "    \n",
    "    qa_pairs.extend(explanatory_qa)\n",
    "    \n",
    "    # Add metadata\n",
    "    for i, qa in enumerate(qa_pairs):\n",
    "        qa[\"id\"] = f\"f1_qa_{i:04d}\"\n",
    "        qa[\"created_at\"] = datetime.now().isoformat()\n",
    "    \n",
    "    return qa_pairs\n",
    "\n",
    "# Create dataset\n",
    "print(\"üìä Creating Q-A dataset...\")\n",
    "qa_dataset = create_f1_dataset(f1_data)\n",
    "\n",
    "print(f\"‚úÖ Created {len(qa_dataset)} Q-A pairs\")\n",
    "\n",
    "# Show examples\n",
    "print(\"\\nüìù Example Q-A pairs:\")\n",
    "for i, qa in enumerate(qa_dataset[:3]):\n",
    "    print(f\"\\n{i+1}. [{qa['type'].upper()}] {qa['question']}\")\n",
    "    print(f\"   Answer: {qa['answer'][:100]}...\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9363ab0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Format dataset for training\n",
    "def format_for_training(qa_pairs):\n",
    "    \"\"\"Format Q-A pairs for instruction tuning.\"\"\"\n",
    "    formatted_data = []\n",
    "    \n",
    "    instruction_template = \"\"\"Below is an instruction that describes a task. Write a response that appropriately completes the request.\n",
    "\n",
    "### Instruction:\n",
    "You are an expert Formula 1 assistant. Answer the following question accurately and concisely.\n",
    "\n",
    "{question}\n",
    "\n",
    "### Response:\n",
    "{answer}\"\"\"\n",
    "    \n",
    "    for qa in qa_pairs:\n",
    "        formatted_text = instruction_template.format(\n",
    "            question=qa[\"question\"],\n",
    "            answer=qa[\"answer\"]\n",
    "        )\n",
    "        formatted_data.append({\"text\": formatted_text})\n",
    "    \n",
    "    return formatted_data\n",
    "\n",
    "# Format data\n",
    "formatted_data = format_for_training(qa_dataset)\n",
    "\n",
    "# Create train/test split\n",
    "random.shuffle(formatted_data)\n",
    "split_idx = int(len(formatted_data) * 0.9)  # 90% train, 10% test\n",
    "\n",
    "train_data = formatted_data[:split_idx]\n",
    "test_data = formatted_data[split_idx:]\n",
    "\n",
    "print(f\"üìà Training examples: {len(train_data)}\")\n",
    "print(f\"üìâ Test examples: {len(test_data)}\")\n",
    "\n",
    "# Convert to HuggingFace Dataset\n",
    "train_dataset = Dataset.from_list(train_data)\n",
    "test_dataset = Dataset.from_list(test_data)\n",
    "\n",
    "print(\"‚úÖ Dataset formatted for training\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c731de11",
   "metadata": {},
   "source": [
    "## 4Ô∏è‚É£ Model Setup\n",
    "\n",
    "Load Gemma-3 model and configure LoRA adapters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "113fd4cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model configuration\n",
    "model_name = \"google/gemma-2-2b\"  # Using smaller Gemma model for Colab\n",
    "max_seq_length = 512  # Sequence length\n",
    "\n",
    "print(f\"ü§ñ Loading model: {model_name}\")\n",
    "\n",
    "# Load model with Unsloth\n",
    "model, tokenizer = FastLanguageModel.from_pretrained(\n",
    "    model_name=model_name,\n",
    "    max_seq_length=max_seq_length,\n",
    "    dtype=None,  # Auto-detect\n",
    "    load_in_4bit=True,  # 4-bit quantization for memory efficiency\n",
    ")\n",
    "\n",
    "print(\"‚úÖ Base model loaded\")\n",
    "\n",
    "# Configure LoRA\n",
    "print(\"‚öôÔ∏è  Configuring LoRA adapters...\")\n",
    "\n",
    "model = FastLanguageModel.get_peft_model(\n",
    "    model,\n",
    "    r=8,  # LoRA rank (higher for better performance)\n",
    "    target_modules=[\"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\",\n",
    "                    \"gate_proj\", \"up_proj\", \"down_proj\"],\n",
    "    lora_alpha=16,  # LoRA scaling\n",
    "    lora_dropout=0.1,  # LoRA dropout\n",
    "    bias=\"none\",  # No bias training\n",
    "    use_gradient_checkpointing=\"unsloth\",  # Memory optimization\n",
    "    random_state=42,\n",
    ")\n",
    "\n",
    "print(\"‚úÖ LoRA adapters configured\")\n",
    "print(f\"üìä Model memory footprint: ~{torch.cuda.max_memory_allocated() / 1024**3:.1f} GB\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4163d4d9",
   "metadata": {},
   "source": [
    "## 5Ô∏è‚É£ Training\n",
    "\n",
    "Fine-tune the model with our F1 dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cedcb587",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training configuration\n",
    "training_args = TrainingArguments(\n",
    "    per_device_train_batch_size=4,      # Batch size\n",
    "    gradient_accumulation_steps=2,       # Gradient accumulation\n",
    "    warmup_steps=50,                     # Warmup steps\n",
    "    num_train_epochs=3,                  # Number of epochs\n",
    "    learning_rate=2e-4,                  # Learning rate\n",
    "    fp16=not torch.cuda.is_available(),  # Use FP16 if CUDA available\n",
    "    bf16=torch.cuda.is_available(),      # Use BF16 on modern GPUs\n",
    "    logging_steps=5,                     # Log every N steps\n",
    "    optim=\"adamw_8bit\",                  # 8-bit optimizer\n",
    "    weight_decay=0.01,                   # Weight decay\n",
    "    lr_scheduler_type=\"cosine\",          # Learning rate scheduler\n",
    "    seed=42,                             # Random seed\n",
    "    output_dir=\"./f1_model_output\",      # Output directory\n",
    "    save_steps=50,                       # Save every N steps\n",
    "    save_total_limit=2,                  # Keep only 2 checkpoints\n",
    "    report_to=None,                      # Disable logging to external services\n",
    "    remove_unused_columns=False,\n",
    ")\n",
    "\n",
    "# Create trainer\n",
    "trainer = SFTTrainer(\n",
    "    model=model,\n",
    "    tokenizer=tokenizer,\n",
    "    train_dataset=train_dataset,\n",
    "    dataset_text_field=\"text\",\n",
    "    max_seq_length=max_seq_length,\n",
    "    dataset_num_proc=1,\n",
    "    packing=False,\n",
    "    args=training_args,\n",
    ")\n",
    "\n",
    "print(\"‚úÖ Trainer configured\")\n",
    "print(f\"üìä Training steps per epoch: {len(train_dataset) // (training_args.per_device_train_batch_size * training_args.gradient_accumulation_steps)}\")\n",
    "print(f\"‚è±Ô∏è  Estimated training time: ~15-25 minutes\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52e4e206",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Start training\n",
    "print(\"üöÄ Starting training...\")\n",
    "print(\"üìà Training progress will be shown below:\")\n",
    "\n",
    "start_time = time.time()\n",
    "\n",
    "# Train the model\n",
    "trainer.train()\n",
    "\n",
    "end_time = time.time()\n",
    "training_duration = (end_time - start_time) / 60  # Convert to minutes\n",
    "\n",
    "print(f\"\\n‚úÖ Training completed in {training_duration:.1f} minutes!\")\n",
    "print(f\"üéâ Model fine-tuned successfully!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca0bef4c",
   "metadata": {},
   "source": [
    "## 6Ô∏è‚É£ Testing & Evaluation\n",
    "\n",
    "Let's test our fine-tuned model with some F1 questions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9bc36eff",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test function\n",
    "def test_f1_model(question):\n",
    "    \"\"\"Test the model with a question.\"\"\"\n",
    "    prompt = f\"\"\"Below is an instruction that describes a task. Write a response that appropriately completes the request.\n",
    "\n",
    "### Instruction:\n",
    "You are an expert Formula 1 assistant. Answer the following question accurately and concisely.\n",
    "\n",
    "{question}\n",
    "\n",
    "### Response:\n",
    "\"\"\"\n",
    "    \n",
    "    # Tokenize\n",
    "    inputs = tokenizer([prompt], return_tensors=\"pt\").to(\"cuda\")\n",
    "    \n",
    "    # Generate\n",
    "    with torch.no_grad():\n",
    "        outputs = model.generate(\n",
    "            **inputs,\n",
    "            max_new_tokens=128,\n",
    "            temperature=0.7,\n",
    "            do_sample=True,\n",
    "            pad_token_id=tokenizer.eos_token_id,\n",
    "        )\n",
    "    \n",
    "    # Decode response\n",
    "    response = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "    \n",
    "    # Extract answer\n",
    "    if \"### Response:\" in response:\n",
    "        answer = response.split(\"### Response:\")[1].strip()\n",
    "    else:\n",
    "        answer = response.strip()\n",
    "    \n",
    "    return answer\n",
    "\n",
    "# Test questions\n",
    "test_questions = [\n",
    "    \"What is Formula 1?\",\n",
    "    \"How does DRS work in Formula 1?\",\n",
    "    \"What are F1 tyre compounds?\",\n",
    "    \"Who is Lewis Hamilton?\",\n",
    "    \"How does F1 qualifying work?\"\n",
    "]\n",
    "\n",
    "print(\"üß™ Testing F1 Expert Model\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "for i, question in enumerate(test_questions, 1):\n",
    "    print(f\"\\n{i}. üèéÔ∏è  Question: {question}\")\n",
    "    answer = test_f1_model(question)\n",
    "    print(f\"   ü§ñ Answer: {answer}\")\n",
    "    print(\"-\" * 50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e322b404",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Interactive testing\n",
    "print(\"üéÆ Interactive Testing - Ask your own F1 questions!\")\n",
    "print(\"(Leave empty and run to skip this section)\")\n",
    "\n",
    "# You can modify this cell to test with your own questions\n",
    "custom_questions = [\n",
    "    # Add your questions here, for example:\n",
    "    # \"Who won the 2023 Monaco Grand Prix?\",\n",
    "    # \"What is the current F1 points system?\",\n",
    "]\n",
    "\n",
    "if custom_questions:\n",
    "    for question in custom_questions:\n",
    "        if question.strip():\n",
    "            print(f\"\\nüèéÔ∏è  Your Question: {question}\")\n",
    "            answer = test_f1_model(question)\n",
    "            print(f\"ü§ñ F1 Expert: {answer}\")\n",
    "            print(\"-\" * 40)\n",
    "else:\n",
    "    print(\"No custom questions provided. Skipping interactive testing.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "105a6bbf",
   "metadata": {},
   "source": [
    "## 7Ô∏è‚É£ Model Saving\n",
    "\n",
    "Save the trained model for later use."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9da72b69",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save model and tokenizer\n",
    "output_dir = \"./gemma_f1_expert\"\n",
    "\n",
    "print(f\"üíæ Saving model to: {output_dir}\")\n",
    "\n",
    "# Save LoRA adapters and tokenizer\n",
    "model.save_pretrained(output_dir)\n",
    "tokenizer.save_pretrained(output_dir)\n",
    "\n",
    "# Save training metadata\n",
    "metadata = {\n",
    "    \"base_model\": model_name,\n",
    "    \"lora_rank\": 8,\n",
    "    \"lora_alpha\": 16,\n",
    "    \"training_examples\": len(train_dataset),\n",
    "    \"training_time_minutes\": training_duration,\n",
    "    \"training_date\": datetime.now().isoformat(),\n",
    "    \"framework\": \"unsloth\",\n",
    "    \"description\": \"F1 expert model trained on racing Q-A data\"\n",
    "}\n",
    "\n",
    "with open(f\"{output_dir}/training_metadata.json\", 'w') as f:\n",
    "    json.dump(metadata, f, indent=2)\n",
    "\n",
    "print(\"‚úÖ Model saved successfully!\")\n",
    "print(f\"üìÅ Model location: {output_dir}\")\n",
    "print(f\"üìä Model size: ~{sum(f.stat().st_size for f in Path(output_dir).glob('**/*') if f.is_file()) / 1024**2:.1f} MB\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8e0a187",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Optional: Download model files\n",
    "print(\"üì• Creating downloadable model archive...\")\n",
    "\n",
    "# Create a zip file of the model\n",
    "!zip -r gemma_f1_expert.zip ./gemma_f1_expert/\n",
    "\n",
    "print(\"‚úÖ Model archive created: gemma_f1_expert.zip\")\n",
    "print(\"üí° You can download this file from the Colab file browser\")\n",
    "\n",
    "# Show file sizes\n",
    "!ls -lh gemma_f1_expert.zip\n",
    "!ls -lh ./gemma_f1_expert/"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "953eeaa3",
   "metadata": {},
   "source": [
    "## 8Ô∏è‚É£ Optional: Upload to Hugging Face Hub\n",
    "\n",
    "Upload your model to Hugging Face Hub for easy sharing and deployment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c09e3256",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Optional: Upload to Hugging Face Hub\n",
    "# Uncomment and modify the following code to upload your model\n",
    "\n",
    "\"\"\"\n",
    "# Install huggingface_hub\n",
    "!pip install -q huggingface_hub\n",
    "\n",
    "from huggingface_hub import notebook_login\n",
    "\n",
    "# Login to Hugging Face (you'll need to enter your token)\n",
    "notebook_login()\n",
    "\n",
    "# Upload model\n",
    "hub_model_id = \"your-username/gemma-f1-expert\"  # Change this to your desired model name\n",
    "\n",
    "model.push_to_hub(hub_model_id, token=True)\n",
    "tokenizer.push_to_hub(hub_model_id, token=True)\n",
    "\n",
    "print(f\"‚úÖ Model uploaded to: https://huggingface.co/{hub_model_id}\")\n",
    "\"\"\"\n",
    "\n",
    "print(\"üí° To upload to Hugging Face Hub:\")\n",
    "print(\"1. Uncomment the code above\")\n",
    "print(\"2. Change 'your-username/gemma-f1-expert' to your desired model name\")\n",
    "print(\"3. Run the cell and follow the login prompts\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db21c5de",
   "metadata": {},
   "source": [
    "## üéâ Congratulations!\n",
    "\n",
    "You've successfully fine-tuned a Gemma-3 model into an F1 expert! Here's what you've accomplished:\n",
    "\n",
    "### ‚úÖ What you built:\n",
    "- **F1 Data Collection**: Gathered race results from Jolpica API\n",
    "- **Custom Dataset**: Created ~100+ F1 question-answer pairs\n",
    "- **Model Fine-tuning**: Applied LoRA to Gemma-3 for F1 expertise\n",
    "- **Testing & Validation**: Verified model performance on F1 questions\n",
    "- **Model Export**: Saved model for deployment\n",
    "\n",
    "### üöÄ Next Steps:\n",
    "1. **Download your model** from the file browser\n",
    "2. **Test with more questions** to evaluate performance\n",
    "3. **Deploy locally** using the CLI script from the repository\n",
    "4. **Build a web app** with Streamlit for interactive use\n",
    "5. **Expand the dataset** with more F1 data for better performance\n",
    "\n",
    "### üìö Key Learnings:\n",
    "- LoRA enables efficient fine-tuning on limited hardware\n",
    "- Quality dataset creation is crucial for good model performance\n",
    "- Instruction tuning format works well for Q-A tasks\n",
    "- Unsloth significantly speeds up training and reduces memory usage\n",
    "\n",
    "### üîó Resources:\n",
    "- [Unsloth Documentation](https://github.com/unslothai/unsloth)\n",
    "- [Jolpica-F1 API](https://jolpi.ca/)\n",
    "- [Hugging Face Transformers](https://huggingface.co/docs/transformers)\n",
    "- [PEFT Library](https://huggingface.co/docs/peft)\n",
    "\n",
    "---\n",
    "üèéÔ∏è **Happy Racing with AI!** üèÅ"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
